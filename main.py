import os
import tempfile
from utils.file_loader import load_file
from agents.quality_agent import run_quality_agent
from agents.static_analysis_agent import run_static_analysis
from agents.error_comparator_agent import compare_issues
from controls.recursive_controller import build_langgraph_loop
from utils.context_analyzer import analyze_project_context
from dotenv import load_dotenv


def format_initial_analysis_report(quality_results, static_results, merged_issues, code_path):
    score = quality_results.get("score", 0)
    ai_issues = quality_results.get("issues", [])

    total_issues = len(merged_issues)
    ai_only_issues = len([i for i in merged_issues if i.get("source") == "AI"])
    static_only_issues = len([i for i in merged_issues if i.get("source") == "Static"])
    both_issues = len([i for i in merged_issues if i.get("source") == "Both"])

    high_severity = [i for i in merged_issues if i.get("severity") == "high"]
    medium_severity = [i for i in merged_issues if i.get("severity") == "medium"]
    low_severity = [i for i in merged_issues if i.get("severity") == "low"]

    report = f"""
{'=' * 80}
ðŸ” AI CODE REVIEWER - INITIAL ANALYSIS REPORT
{'=' * 80}

ðŸ“ File Analyzed: {code_path}
ðŸ“Š Overall Quality Score: {score}/100
ðŸ•’ Analysis Timestamp: {__import__('datetime').datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

{'=' * 80}
ðŸ“ˆ SUMMARY STATISTICS
{'=' * 80}
â€¢ Total Issues Found: {total_issues}
â€¢ AI Analysis Issues: {ai_only_issues}
â€¢ Static Analysis Issues: {static_only_issues}
â€¢ Confirmed Issues (Both): {both_issues}

ðŸ“Š SEVERITY BREAKDOWN:
â€¢ ðŸ”´ High Priority: {len(high_severity)} issues
â€¢ ðŸŸ¡ Medium Priority: {len(medium_severity)} issues
â€¢ ðŸŸ¢ Low Priority: {len(low_severity)} issues

{'=' * 80}
ðŸ” DETAILED ISSUE ANALYSIS
{'=' * 80}
"""

    if not merged_issues:
        report += "âœ… No issues detected! Your code appears to be clean.\n"
    else:
        sources = {"AI": [], "Static": [], "Both": []}
        for issue in merged_issues:
            source = issue.get("source", "AI")
            sources[source].append(issue)

        for source, issues in sources.items():
            if issues:
                source_icon = "ðŸ¤–" if source == "AI" else "ðŸ”§" if source == "Static" else "ðŸ¤"
                report += f"\n{source_icon} {source.upper()} ANALYSIS ISSUES ({len(issues)} found):\n"
                report += "-" * 60 + "\n"

                for i, issue in enumerate(issues, 1):
                    line_num = issue.get("line", "N/A")
                    description = issue.get("description", "No description")
                    suggestion = issue.get("suggestion", "No suggestion")
                    severity = issue.get("severity", "medium")
                    confidence = issue.get("confidence", 0.8)
                    severity_icon = "ðŸ”´" if severity == "high" else "ðŸŸ¡" if severity == "medium" else "ðŸŸ¢"

                    report += f"{i:2d}. Line {line_num:3d} | {severity_icon} {severity.upper()}\n"
                    report += f"    ðŸ“ Issue: {description}\n"
                    report += f"    ðŸ’¡ Suggestion: {suggestion}\n"
                    report += f"    ðŸŽ¯ Confidence: {confidence:.1%}\n"
                    report += "\n"

    report += f"{'=' * 80}\n"
    report += "ðŸ“Š QUALITY SCORE INTERPRETATION\n"
    report += "=" * 80 + "\n"

    if score >= 90:
        report += "ðŸ† EXCELLENT (90-100): Code follows best practices excellently!\n"
    elif score >= 80:
        report += "âœ… GOOD (80-89): Code is well-structured with minor improvements possible.\n"
    elif score >= 70:
        report += "âš ï¸  FAIR (70-79): Code needs some improvements but is generally acceptable.\n"
    elif score >= 60:
        report += "ðŸ”§ NEEDS WORK (60-69): Several issues need to be addressed.\n"
    else:
        report += "ðŸš¨ POOR (0-59): Significant refactoring required.\n"

    report += f"\nðŸŽ¯ RECOMMENDATIONS:\n"
    if total_issues == 0:
        report += "â€¢ Your code is in excellent condition!\n"
        report += "â€¢ Consider running optimization for performance improvements.\n"
    elif len(high_severity) > 0:
        report += f"â€¢ ðŸ”´ Address {len(high_severity)} high-priority issues first.\n"
    if len(medium_severity) > 0:
        report += f"â€¢ ðŸŸ¡ Review {len(medium_severity)} medium-priority issues.\n"
    if score < 80:
        report += "â€¢ Consider running iterative optimization to improve code quality.\n"

    report += f"\n{'=' * 80}\n"
    return report


def format_iteration_summary(final):
    report = f"""
{'=' * 80}
ðŸŽ¯ ITERATIVE OPTIMIZATION COMPLETE
{'=' * 80}

ðŸ“Š FINAL RESULTS:
â€¢ Total Iterations: {len(final.get('history', []))}
â€¢ Best Quality Score: {final.get('best_score', 'N/A')}
â€¢ Final Code Length: {len(final.get('best_code', ''))} characters
â€¢ Total Issues Fixed: {sum(h.get('issues_fixed', 0) for h in final.get('history', []))}

{'=' * 80}
ðŸ“š ITERATION HISTORY
{'=' * 80}
"""

    for i, step in enumerate(final.get("history", []), 1):
        report += f"\nðŸ§¾ Iteration {step.get('iteration')}:\n"
        report += f"   ðŸ“Š Quality Score: {step.get('score', 'N/A')}\n"
        report += f"   ðŸ” Issues Remaining: {step.get('issue_count', 0)}\n"
        report += f"   âœ… Issues Fixed: {step.get('issues_fixed', 0)}\n"
        report += f"   ðŸ”´ High-Severity Issues: {step.get('high_severity_count', 0)}\n"
        report += f"   ðŸš€ Optimization Applied: {'Yes' if step.get('optimization_applied') else 'No'}\n"
        report += f"   ðŸ“ Code Preview: {step.get('refactored_code', '')}\n"
        report += "-" * 40 + "\n"

    report += f"\n{'=' * 80}\n"
    report += "âœ… FINAL REFACTORED CODE\n"
    report += "=" * 80 + "\n"
    report += final.get("best_code", "[No final code]")
    report += f"\n{'=' * 80}\n"

    return report


def main():
    load_dotenv()
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        print("âŒ GEMINI_API_KEY environment variable not set.")
        return

    code_path = input("ðŸ“„ Enter the path to your code file: ").strip()
    if not os.path.exists(code_path):
        print("âŒ File not found.")
        return

    code = load_file(code_path)
    project_dir = os.path.dirname(code_path) or "."
    context = analyze_project_context(project_dir)

    print("\nðŸ” Phase 1: Running Initial Analysis...")
    quality_results = run_quality_agent(code, api_key, context)
    score = quality_results.get("score", 0)

    with tempfile.NamedTemporaryFile(suffix=".py", delete=False, mode="w") as temp_file:
        temp_file.write(code)
        temp_path = temp_file.name
    static_results = run_static_analysis(temp_path)
    os.remove(temp_path)

    merged_issues = compare_issues(quality_results, static_results)
    report = format_initial_analysis_report(quality_results, static_results, merged_issues, code_path)
    print(report)

    answer = input("\nðŸ¤– Apply fixes and optimize code iteratively? (y/N): ").strip().lower()
    if answer != "y":
        print("\nðŸš« Exiting after initial review. No changes applied.")
        return

    print("\nâ™»ï¸ Entering Iterative Optimization Mode...\n")
    state = {
        "api_key": api_key,
        "code": code,
        "iteration": 0,
        "continue_": True,
        "best_code": code,
        "best_score": score,
        "best_issues": merged_issues,
        "issue_count": len(merged_issues),
        "issues_fixed": 0,
        "feedback": [],
        "min_score_threshold": 95.0,
        "max_high_severity_issues": 0,
        "max_iterations": 5,
        "context": context,
        "optimization_applied": False
    }

    graph = build_langgraph_loop()
    final = graph.invoke(state)
    final_report = format_iteration_summary(final)
    print(final_report)
    print("\nðŸ“š Session Summary:")
    from memory.session_memory import show_session_summary
    show_session_summary()


if __name__ == "__main__":
    main()